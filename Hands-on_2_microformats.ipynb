{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************************************************\n",
    "#  The Social Web \n",
    "- Instructors: Davide Ceolin, Dayana Spagnuelo\n",
    "- Lab Assistants: Michael Accetto, Sarthak Gupta \n",
    "- Exercises for Hands-on session 2\n",
    "- 12 February 2020 11:00 - 12:45                 \n",
    "- NU-5B-21, NU-6A-04, NU-6B-20, NU-6C-39, NU-6C-40                             \n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you are going to mine data in various microformats. You will see the differences in what each of the formats can contain and what purpose they serve. We will start by looking at geographical data. \n",
    "\n",
    "Prerequisites:\n",
    "- Python 3.7\n",
    "- Python packages: requests, BeautifulSoup4, HTMLParser, rdflib, rdflib_microdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from requests)\n",
      "Requirement already satisfied: BeautifulSoup4 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from BeautifulSoup4)\n",
      "Requirement already satisfied: HTMLParser in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages\n",
      "Requirement already satisfied: rdflib in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages\n",
      "Requirement already satisfied: isodate in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib)\n",
      "Requirement already satisfied: pyparsing in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib)\n",
      "Requirement already satisfied: six in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from isodate->rdflib)\n"
     ]
    }
   ],
   "source": [
    "# If you're using a virtualenv, make sure it's activated before running \n",
    "# this cell!\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup4\n",
    "!pip install HTMLParser \n",
    "!pip install rdflib\n",
    "# !pip install rdflib_microdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the newer features available with the facebook-sdk package, we will install the package from its github repository. \\\n",
    "This is possible with pip, by following this syntax:  \n",
    "\n",
    "`pip install git+YOUR_GITHUB_REPOSITORY_URL` \n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining rdflib-microdata from git+https://github.com/edsu/rdflib-microdata.git#egg=rdflib-microdata\n",
      "  Updating /home/mknw/.venv/tswvenv37/src/rdflib-microdata clone\n",
      "Requirement already satisfied: html5lib in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib-microdata)\n",
      "Requirement already satisfied: microdata>=0.2 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib-microdata)\n",
      "Requirement already satisfied: rdflib>=3.0 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib-microdata)\n",
      "Requirement already satisfied: webencodings in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from html5lib->rdflib-microdata)\n",
      "Requirement already satisfied: six>=1.9 in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from html5lib->rdflib-microdata)\n",
      "Requirement already satisfied: isodate in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib>=3.0->rdflib-microdata)\n",
      "Requirement already satisfied: pyparsing in /home/mknw/.venv/tswvenv37/lib/python3.7/site-packages (from rdflib>=3.0->rdflib-microdata)\n",
      "Installing collected packages: rdflib-microdata\n",
      "  Found existing installation: rdflib-microdata 0.2.0\n",
      "    Uninstalling rdflib-microdata-0.2.0:\n",
      "      Successfully uninstalled rdflib-microdata-0.2.0\n",
      "  Running setup.py develop for rdflib-microdata\n",
      "Successfully installed rdflib-microdata\n"
     ]
    }
   ],
   "source": [
    "!pip install -e git+https://github.com/edsu/rdflib-microdata.git#egg=rdflib-microdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 1 \n",
    "Extracting coordinates from a webpage and reformatting them in the geo microformat (based on Example 8-1 in Mining the Social Web). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This script requires you to add a url of a page with geotags to the commandline, e.g.\n",
    "# python geo.py 'http://en.wikipedia.org/wiki/Amsterdam'\n",
    "URL = 'https://en.wikipedia.org/wiki/Amsterdam'\n",
    "\n",
    "req = requests.get(URL, headers={'User-Agent' : \"Social Web Course Student\"})\n",
    "soup = BeautifulSoup(req.text)\n",
    "# print(req.text)\n",
    "placemark = soup.findAll(\"Placemark\")\n",
    "print(placemark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geoTag = soup.find(True, 'geo')\n",
    "\n",
    "if geoTag and len(geoTag) > 1:\n",
    "        lat = geoTag.find(True, 'latitude').string\n",
    "        lon = geoTag.find(True, 'longitude').string\n",
    "        print ('Location is at'), lat, lon\n",
    "elif geoTag and len(geoTag) == 1:\n",
    "        (lat, lon) = geoTag.string.split(';')\n",
    "        (lat, lon) = (lat.strip(), lon.strip())\n",
    "        print (('Location is at'), lat, lon)\n",
    "else:\n",
    "        print ('Location not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Can you convert the output of Exercise 1 into KML? Here is the KML documentation: https://developers.google.com/kml/documentation/?csw=1 and here you can find a simple example of how it is used: https://renenyffenegger.ch/notes/tools/Google-Earth/kml/index\n",
    "\n",
    "Visualise the point in Google Maps using the following code example: https://developers.google.com/maps/documentation/javascript/examples/layer-kml-features\n",
    "You will have to create your own KML file for the custom map layer, and provide a URL to the KML file inside the JavaScript code, which means that you have to upload the file somewhere. You can use a service like http://pastebin.com/ to obtain a URL for your KML file —> paste the code there and request the RAW format URL; use this one in this Task1.\n",
    "\n",
    "Is KML a microformat, why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "In order to find information in the web we can use microformats. However in this example you will not be using hRecipe. Instead, we'll show you how to find arbitrary tags in a webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 \n",
    "Parsing data for a <sub><sup>veggie</sup></sub> spaghetti alla carbonara recipe (from Example 2-7 in Mining the Social Web)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# A yummy webpage (feel free to change to your likings.)\n",
    "URL = \"https://www.acouplecooks.com/spring-vegetarian-spaghetti-carbonara/\"\n",
    "\n",
    "# requests will return the html found at the given webpage...\n",
    "page = requests.get(URL)\n",
    "# ...and a BeautifulSoup object can be created from its content.\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "listchildren = list(soup.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find any element in the page through *html tag selectors*\n",
    "You can find them all [here](https://www.w3schools.com/cssref/css_selectors.asp), but shortly these are \".\" for classes, # for ids and plain text for the element name.\n",
    "\n",
    "\n",
    "You can also combine them, so that looking for \".class1.class2\" would select all elements displaying both classes. For a deeper overview please check the above link (or google \"html tag selectors\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[<li><span data-amount=\"1\">1</span> pound spaghetti noodles</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> smoked mozzarella cheese</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> grated Parmesan cheese, plus more for serving</li>, <li><span data-amount=\"4\">4</span> egg yolks</li>, <li><span data-amount=\"1\" data-unit=\"cup\">1 cup</span> frozen Earthbound Farm Organic peas</li>, <li><span data-amount=\"8\" data-unit=\"cup\">8 cup</span>s Earthbound Farm Organic spinach</li>, <li><span data-amount=\"3\" data-unit=\"tablespoon\">3 tablespoon</span>s butter</li>, <li><a data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" target=\"_blank\">Kosher salt</a></li>, <li>Fresh ground black pepper</li>]\n"
     ]
    }
   ],
   "source": [
    "print(len(listchildren)) # we can see here how many children the html doc has got.\n",
    "ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "# let's get all the \"list item\" elements in a list:\n",
    "ing_unp = ingredients_unparsed.findAll('li')\n",
    "print(ing_unp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmh... not so pretty yet. How about listing their items using the text method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "\n",
      "1 pound spaghetti noodles\n",
      "½ cup smoked mozzarella cheese\n",
      "½ cup grated Parmesan cheese, plus more for serving\n",
      "4 egg yolks\n",
      "1 cup frozen Earthbound Farm Organic peas\n",
      "8 cups Earthbound Farm Organic spinach\n",
      "3 tablespoons butter\n",
      "Kosher salt\n",
      "Fresh ground black pepper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ingredients = [t.text for t in ing_unp]\n",
    "print(\"Ingredients:\\n\")\n",
    "[print(i) for i in ingredients]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>In a large pot, combine 6 quarts of water with 2 tablespoons <a data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" target=\"_blank\">kosher salt</a> and bring it to a boil.</li>, <li>Grate the Parmesan and mozzarella cheese. Carefully separate four egg yolks and set aside.</li>, <li>Once boiling, add the pasta and cook until the pasta is just about al dente, about 7 minutes; then add peas and spinach and cook for 1 minute. Reserve 1 cup cooking water, and then drain the pasta and vegetables.</li>, <li>In a skillet, melt the butter, then stir in the cheeses, ¼ cup pasta water, and ¼ teaspoon <a data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" target=\"_blank\">kosher salt</a>. Stir in the pasta and vegetables until creamy over low heat, adding more pasta water if necessary (note that the mozzarella will stick together in some places).</li>, <li>To serve, top each pasta serving with a whole egg yolk and additional Parmesan cheese, and stir the yolk into the pasta at the table (if you are uncomfortable serving egg yolks at the table, stir the egg yolks into the pasta in the skillet to heat them through). Serve immediately. (Note that the mozzarella cheese can become gummy the longer the pasta sits, so eat immediately if possible. Leftovers can be reheated in a skillet, but may not have the same creamy texture.)</li>]\n"
     ]
    }
   ],
   "source": [
    "instructions_unparsed = soup.select_one(\".tasty-recipes-instructions\")\n",
    "instructions_unparsed = instructions_unparsed.findAll(\"li\")\n",
    "print(instructions_unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish off with the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vegetarian Carbonara'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_unparsed = soup.select_one(\".post-header\") # \n",
    "categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "recipe_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1\n",
    "Now it's your turn. Create a function that can scrape any recipe webpage from the same website (other websites will have different class tags). \n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- return itemized content (e.g. ingredients) in a list. You may want to use a list comprehension here.\n",
    "- Not all items have been cleaned of their html markdown (see variables ```ingredients``` vs. ```instructions_unparsed```. Make sure to return a list with human readable content (i.e. by using the ```.text``` attribute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-241c736c9b22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             }\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-241c736c9b22>\u001b[0m in \u001b[0;36mparse_website\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     return {\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34m'ingredients'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mingredients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;34m'instructions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pass in a URL containing hRecipe, such as\n",
    "# https://www.jamieoliver.com/recipes/pasta-recipes/veggie-carbonara/\n",
    "\n",
    "URL = \"https://www.acouplecooks.com/\"#YOUR RECIPE HERE/\n",
    "\n",
    "# Parse out some of the pertinent information for a recipe.\n",
    "# See http://microformats.org/wiki/hrecipe.\n",
    "\n",
    "def parse_website(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # You code here\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # title = \n",
    "    # ingredients_unparsed = \n",
    "    # instructions = \n",
    "\n",
    "    return {\n",
    "            'name': fn,\n",
    "            'ingredients': ingredients,\n",
    "            'instructions': instructions,\n",
    "            }\n",
    "    \n",
    "recipe = parse_website(URL)\n",
    "print (recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But How can we get information not only from one website,  but from all? \n",
    "\n",
    "The answer: microformats.\n",
    "\n",
    "But rather than extracting with information manually from the schema.org or hRecipe microformats, we can use a package, ```scrape-schema-recipe``` \n",
    "\n",
    "Feel free to experiment with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "hRecipe is a microformat specifically created for recipes.\n",
    "Can you for example easily compare different dessert recipe ingredients? For inspiration you can look back at the exercises you did in Hands-on session 1 where you compared different sets of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema.org is one of the most widely used annotations formats. Schema.org is a multipurpose  template that has been created by a consortium consisting of Yahoo!, Google and Microsoft. It can describe entities, events, products etc. Check out the vocabulary specs on Schema.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Parsing schema.org microdata. To parse this data you need to install the rdflib-microdata package, which you have done in one of the previous steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import rdflib\n",
    "import rdflib_microdata\n",
    "\n",
    "# Pass in a URL containing Schema.org microformats\n",
    "url = \"http://www.last.fm/music/Red+Hot+Chili+Peppers?ac=red\"\n",
    "\n",
    "g = rdflib.Graph()\n",
    "g.parse(url, format=\"microdata\")\n",
    "print (g.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 \n",
    "Compare the schema.org information about a band on last.fm to the Facebook Open Graph information about the same band from Facebook. What are the differences? Which format do you think supports better interoperability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2\n",
    "Explore the various microformats at http://microformats.org/ and compare the output of the exercises with the output of http://microformats.org/. Think about possible microformats you want to support in your final assignment and read up on how to parse them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
